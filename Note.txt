2.2
小节总结:
    学习了文本嵌入层的作用
    无论是源文本嵌入还是目标文本嵌入，都是为了将文本中词汇的数字表示转换为
    向量表示，希望在这样的高位空间捕捉词汇间的关系

    学习并实现了文本嵌入层的类Embedding
        初始化函数以d_model词嵌入维度，和vocab词汇总数为参数，内部主要使用了nn中预定层Embeeding进行词嵌入
    在forward函数中，将输入x传入到Embedding的实例化对象中，然后乘以一个根号下d_model进行缩放，控制数值大小，他的输出是文本嵌入后的结果

    学习位置编码的作用
        因为在Transformer的编码器结构中，并没有针对词汇位置信息的处理，因此需要在Embedding层后加入位置编码器，将词汇位置不同可能产生不同语义的信息
        加入到词嵌入张量中，以弥补位置信息的确实

    学习并实现了位置编码器的类：PositionalEncoding
        初始化函数以d_model,dropout,max_len为参数，分别代表d_model:词嵌入维度，dropout置0比例，max_len为参数，代表每个句子最大长度
        forward函数中的输入参数时x,是Embedding层的输出
        最终输出一个加入位置编码信息的词嵌入张量

    实现词汇向量中特征的分布曲线：
        保证同一个词汇随着所在位置不同它对应位置嵌入向量会发生变化
        正弦波和余弦波的值域范围都是1到-1，这有很好的控制嵌入数值的大小，有助于梯度的快速计算


2.3 编码器的实现
    学习目标：
        了解编码器中各个组成部分的作用
        掌握编码器各个组成部分的实现过程
    编码器部分
        由N个编码器层堆叠而成
        每个编码器层由两个子层连接结构而成
        第一个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接
        第二个子层连接结构包括一个前馈全连接子层和规范化以及一个残差连接
2.3.1 掩码张量

    了解什么是掩码张量以及它的作用
    掌握生成张量的实现过程

    什么是掩码张量？
        掩代表遮掩，码就是我们张良忠的数值，它的尺寸不定，里面一般只有1和0元素，代表被这样活着不被这样，至于是0位置
    遮掩还是1位置被这样可以自定义，因此它的作用是让另一个张量中的一些数值被遮挡，也可以说被替换，它的表现形式是一个张量

    掩码张量的作用
        在tranformer里面，掩码张量的作用主要是应用在attention时，有些生成的attention张量中的值计算有可能已知了未来的值而得到，
    未来信息被看成是因为训练时会把整个输出结果都一次性进行Embedding,但是理论上解码器的输出却不是一次就能产生最终的结果的，
    而是一次次通过上一次结果过综合得出的因此未来信息可能被提前利用，所以我们进行遮掩。
    学习小结：
            学习并实现了subsequent_mask函数
                输入size,表示矩阵的大侠
                输出是下三角矩阵
                最后可视化分析，更深一步理解它的用途
2.3.2 注意力机制
    学习目标:
        掌握注意力计算规则实现规则
        了解什么是注意力计算规则和注意力机制
    什么是注意力:
        我们观察事务时,之所以能够快速判断一种事物(当然允许判断错误的),是因为我们大脑很快把注意力放在事物最有辨识度的
    的部分从而做出判断，而并非是从头到尾的观察一遍事务之后才能有判断结果，正是基于这样的理论
    就产生了注意力机制

    注意力机制计算规则:
        Attention(Q, K, V) = softmax(QKT/sqrt(dk)) * V

    Q、K、V的比喻解释:
        一个问题,给出一短文本使用一些关键词对它进行描述，
    为了便于同一正确答案，这道题可能预先已经已经给大家写出了一些关键词作为提示
    就可以看作key, 整个的文本信息就相当于是query, value含义则更抽象,可以比
    作是你看到这段文本信息后，脑子里浮现的答案信息，这里我们又假设大家最开始都不是很聪明，
    第一次看到这段文本后脑子里基本上浮现的答案信息就只有提示这些信息，因此key与value基本是相同的，
    但是随着我们对这个问题的深入理解，通过我们思考脑子里想起来的东西原来，
    并且能够对我们query也就是这段文本，提取关键信息进行表示，这就是注意力作用的过程，
    通过这个过程，我们最终脑子里的value发生了变化

2.3.3 多头注意力机制：
    学习目标：
        了解多头注意力机制的作用
        掌握多头注意力机制的实现过程
    什么是多头注意力？
        从多头注意力结构图中，貌似这个所谓的多个头就是指的多组线性变换层，其实并不是
    只有一组使用了线性变换，即对三个变换张量Q, K, V分别进行线性变换，
    这些变换不会改变原有张量的尺寸，因此每个变换矩阵都是方阵，得到输出结果后， 多头的作用
    才开始显现，每个个头开始从词义层面分割输出的张量，也就是每个头都想获得一组Q、K、V进行注意力机制的
    计算，但是句子中的每个词表示只获得一部分，也就是只分割了最后一维的词嵌入向量，这就是所谓的多头，
    将每个头获得的输入送到注意力机制中，就形成了多头注意力。
    多头注意力机制的代码实现:
        这种结构设计能让每个注意力机制去优化每个词汇的不同特征部分，
    从而均衡同一种注意力机制可能产生的偏差，让此役拥有来自更多元的表达，实验表明可以从而提升模型效果

        也就是用Embedding计算出来的词编码矩阵对于前100维度，中间100维度，后面100维度的得到的信息是不一样的，也就是相当于卷积神经网络的不同的通道维数
2.3.4 前馈全连接层
    学习目标:
        了解什什么是前馈全连接层及其它的作用
        掌握前馈全连接层的实现过程
    什么是前馈全连接层：
        在Transformer中前馈全连接就是具有两层线性层的全连接网络
    前馈全连接层的作用
        考虑注意力机制可能对复杂过程的你和程度不够，通过增加两层网络来增强模型的能力
    代码分析
        # 通过类PositionwiseFeedForward来实现前馈全连接层
        class PositionwiseFeedForward(nn.Module):
            def __init__(self, d_model, d_ff, dropout=0.1):
                """初始化函数有三个输入参数分别是d_model, d_ff,和dropout=0.1，第一个是线性层的输入维度也是第二个线性层的输出维度，
                   因为我们希望输入通过前馈全连接层后输入和输出的维度不变. 第二个参数d_ff就是第二个线性层的输入维度和第一个线性层的输出维度.
                   最后一个是dropout置0比率."""
                super(PositionwiseFeedForward, self).__init__()

                # 首先按照我们预期使用nn实例化了两个线性层对象，self.w1和self.w2
                # 它们的参数分别是d_model, d_ff和d_ff, d_model
                self.w1 = nn.Linear(d_model, d_ff)
                self.w2 = nn.Linear(d_ff, d_model)
                # 然后使用nn的Dropout实例化了对象self.dropout
                self.dropout = nn.Dropout(dropout)

            def forward(self, x):
                """输入参数为x，代表来自上一层的输出"""
                # 首先经过第一个线性层，然后使用Funtional中relu函数进行激活,
                # 之后再使用dropout进行随机置0，最后通过第二个线性层w2，返回最终结果.
                return self.w2(self.dropout(F.relu(self.w1(x))))
    2.3.4 前馈全连接层总结:

        学习了什么是前馈全连接层:
            在Transformer中前馈全连接层就是具有两层线性层的全连接网络.

        学习了前馈全连接层的作用:
            考虑注意力机制可能对复杂过程的拟合程度不够, 通过增加两层网络来增强模型的能力.

        学习并实现了前馈全连接层的类: PositionwiseFeedForward
            它的实例化参数为d_model, d_ff, dropout, 分别代表词嵌入维度, 线性变换维度, 和置零比率.
            它的输入参数x, 表示上层的输出.
            它的输出是经过2层线性网络变换的特征表示.

2.3.5 规范化层总结:

    学习了规范化层的作用:
        它是所有深层网络模型都需要的标准网络层，因为随着网络层数的增加，
        通过多层的计算后参数可能开始出现过大或过小的情况，
        这样可能会导致学习过程出现异常，模型可能收敛非常的慢.
        因此都会在一定层数后接规范化层进行数值的规范化，
        使其特征数值在合理范围内.

    学习并实现了规范化层的类: LayerNorm
        它的实例化参数有两个, features和eps，分别表示词嵌入特征大小，和一个足够小的数.
        它的输入参数x代表来自上一层的输出.
        它的输出就是经过规范化的特征表示.

    学习了规范化层的作用:
        它是所有深层网络模型都需要的标准网络层，因为随着网络层数的增加，通过多层的计算后参数可能开始出现过大或过小的情况，这样可能会导致学习过程出现异常，模型可能收敛非常的慢. 因此都会在一定层数后接规范化层进行数值的规范化，使其特征数值在合理范围内.

    学习并实现了规范化层的类: LayerNorm
        它的实例化参数有两个, features和eps，分别表示词嵌入特征大小，和一个足够小的数.
        它的输入参数x代表来自上一层的输出.
        它的输出就是经过规范化的特征表示.

2.4.1 解码器层总结:

    学习了解码器层的作用:
        作为解码器的组成单元, 每个解码器层根据给定的输入向目标方向进行特征提取操作，即解码过程.

    学习并实现了解码器层的类: DecoderLayer
        类的初始化函数的参数有5个, 分别是size，代表词嵌入的维度大小, 同时也代表解码器层的尺寸，第二个是self_attn，多头自注意力对象，也就是说这个注意力机制需要Q=K=V，第三个是src_attn，多头注意力对象，这里Q!=K=V， 第四个是前馈全连接层对象，最后就是droupout置0比率.
        forward函数的参数有4个，分别是来自上一层的输入x，来自编码器层的语义存储变量mermory， 以及源数据掩码张量和目标数据掩码张量.
        最终输出了由编码器输入和目标数据一同作用的特征提取结果.
2.5 输出部分实现
    学习目标
        了解线性层和softmax的作用.
        掌握线性层和softmax的实现过程.
    输出部分包含:
        线性层
        softmax层
        avatar

    线性层的作用
        通过对上一步的线性变化得到指定维度的输出, 也就是转换维度的作用.
    softmax层的作用
        使最后一维的向量中的数字缩放到0-1的概率值域内, 并满足他们的和为1.

    小节总结
        学习了输出部分包含:

            线性层
            softmax层
        线性层的作用:
            通过对上一步的线性变化得到指定维度的输出, 也就是转换维度的作用.
        softmax层的作用:
            使最后一维的向量中的数字缩放到0-1的概率值域内, 并满足他们的和为1.
        学习并实现了线性层和softmax层的类: Generator
            初始化函数的输入参数有两个, d_model代表词嵌入维度, vocab_size代表词表大小.
            forward函数接受上一层的输出.最终获得经过线性层和softmax层处理的结果.
